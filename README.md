# Multimodal-in-multilabel-classification
This is a demo project on multimodal application in multi-label classification.

## Getting Started
For a multi-modal (text description & image input) multi-classification task, a pre-trained BERT model is utilized for word embedding in textual inputs. Additionally, multiple classic computer vision models such as ResNet, ViT, DeiT, Swin-Transformer, etc., are employed. The study involves conducting multidimensional comparisons and ablation experiments within the constraints of model size to validate the effectiveness of the multi-modal approach and to assess the performance of different models on this multi-classification task.

### Prerequisites
Python 3.x version

Check required-libraries.txt in the code folder for all the packages that need to be installed.

Please open an issue if there is any problem.

**Clone the Project:**
```
git clone https://github.com/Pengwei-Yang/Multimodal-in-multilabel-classification.git
```
### Instructions

Unzip the zip file in the data folder.

Open and run the Jupyter Notebook in the code folder, and reset the path to your own.

If you need in-depth instruction on theories, please step into the paper folder.

### Authors
* **Pengwei Yang** -*Main contributor*-
（https://wwww.pengweiyang.com）

* **Chongyangzi Teng** -*Minor contributor*- （https://www.researchgate.net/profile/Chongyangzi-Teng）
* **Mengshen Guo** -*Minor contributor*- （https://www.researchgate.net/profile/Mengshen-Guo-3）

